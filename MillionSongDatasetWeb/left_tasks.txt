<div class="post">
<h2>Tasks</h2><p>
Our main goal is to provide you with data because you know what 
you want to do with it. Still, we give some information regarding
typical MIR tasks below. We hope to provide snippets of code and benchmarks
results to help you getting started. If you want to provide additional
information / link to your code / new results / new tasks, 
please send us an email!
<ol>
<li><a href="#segmentation">Segmentation</a></li>
<li><a href="#automatictagging">Automatic tagging</a></li>
<li><a href="#yearrecognition">Year recognition</a></li>
<li><a href="#imputation">Imputation of missing data</a></li>
<li><a href="#nameanalysis">Artist name, release, song title analysis</a></li>
<li><a href="#preview">Preview audio</a></li>
</ol>
<a name='segmentation'><h3>Segmentation</h3></a>
The goal is to divide a song into meaningfull segments, usually
chorus / verse / bridge or similar. The Echo Nest analysis provides
some estimated segments, but you can use basic Echo Nest features
(chroma or MFCC-like features) on the beat level with your own
algorithms.<br>
Here we use UCSD <a href="http://cosmal.ucsd.edu/cal/projects/segment/DTMsegment_v1.0.tar">code</a> from this
<a href="http://cosmal.ucsd.edu/cal/projects/segment/">project</a>.
We estimate the sections and compare with Echo Nest estimation.<p>

<a name='automatictagging'><h3>Automatic tagging</h3></a>
<a href="http://scholar.google.com/scholar?q=ismir+automatic+tagging">
Automatic tagging</a> of audio is the association of appropriate
keywords to some specific sound segment. In MIR research, this class
can encompass "music genre recognition", "mood detection", and
some aspects of "audio scene analysis". This dataset provides
audio features and tags, therefore is a good set to compare algorithms
on such tasks.
<p>
To get you started, download these indices, the list of unique
terms (Echo Nest tags) <a href="http://labrosa.ee.columbia.edu/projects/millionsong/files/unique_terms.txt">here</a>
and the two databases (for track metadata <a href="http://www.ee.columbia.edu/~thierry/track_metadata.db">here</a>
and for artist tags <a href="http://www.ee.columbia.edu/~thierry/artist_term.db">here</a>). 
From this, you can easily see which artist got what term, find all tracks from that artist, etc.
<p>
MORE DETAILS: we already provide a train/test split among artists! Please use it
so your results are more easily comparable. The two files come with the
code: <a href="https://github.com/tb2332/MSongsDB/blob/master/Tasks_Demos/Tagging/artists_train.txt">train</a> and 
<a href="https://github.com/tb2332/MSongsDB/blob/master/Tasks_Demos/Tagging/artists_test.txt">test</a>.
This split is based on the 300 most used terms in the dataset, ordered list of these terms is available 
<a href="https://github.com/tb2332/MSongsDB/blob/master/Tasks_Demos/Tagging/top_terms.txt">here</a>.
The artists in the test set have 122125 tracks in the dataset (~12%). Overall, 43943 artists out of 44745 have
terms associated with them.
<p>

<a name='yearrecognition'><h3>Year recognition</h3></a>
This is a simple supervised task easy to set up with the dataset.
From audio features, probably "segments_timbre", and possibly some
other information like "energy", "danceability", etc, try to predict
the year or decade when this song was released.<p>

Some code has been created to get you started, see
<a href="https://github.com/tb2332/MSongsDB/tree/master/Tasks_Demos/YearPrediction">YearPrediction</a>
folder. You also want to get (or recreate using track_metadata.db) the list
of all tracks for which we have the year information (515576 tracks): 
<a href="http://labrosa.ee.columbia.edu/projects/millionsong/files/tracks_per_year.txt">tracks_per_year.txt</a>. 
So everyone reports comparable results, we provide a train/test split of
the 28223 artists that have at least one song with year info. The 2822 test artists
authored 51638 tracks with year information, about 10% of the whole set. We split according
to artists, and not according to tracks, to avoid the producer effect. See 
<a href="https://github.com/tb2332/MSongsDB/blob/master/Tasks_Demos/YearPrediction/split_train_test.py">
split_train_test.py</a> for details, and 
<a href="https://github.com/tb2332/MSongsDB/blob/master/Tasks_Demos/YearPrediction/artists_train.txt">train</a>
and
<a href="https://github.com/tb2332/MSongsDB/blob/master/Tasks_Demos/YearPrediction/artists_test.txt">test</a>
the actual split.
<p>

<a name='imputation'><h3>Imputation of missing data</h3></a>
Imputation of missing data in time series is
<a href="http://scholar.google.com/scholar?hl=en&q=interpolation+missing+values+audio+data">well-known</a>.
Recently, we have studied imputation of beat-aligned
chroma features ???? using Echo Nest data. The Million Song
Dataset can easily be use to further experiment with this task.
Code available here ????.<p>

<a name='nameanalysis'><h3>Artist name, release, song title analysis</h3></a>
Many analysis of the metadata is possible. How do the words of the artist
names or their song titles cluster? Can we predict tags based on that, or
is the clustering similar? What is "the most typical song title"
imagineable. See 
<a href="https://github.com/tb2332/MSongsDB/tree/master/Tasks_Demos/NamesAnalysis/">
NamesAnalysis</a> folder for sample python code.<br>
Note that these scripts can
be useful for other tasks, for instance to create the list of all artists
in the dataset (that is how we created the file unique_artists.txt).<p>
<small>
<div style="border:1px solid #808080;padding:1.2em; background-color:#f0f0f0; margin-left: 25px; margin-right: 40px;">
<div style="margin-left: 25px;">
<pre>
python list_all_artists.py DATASETDIR allartists.txt
</pre>
</div>
</div>
</small><p>
The previous code goes through the million songs, which can take hours.
A smarter code would use the SQLite database track_metadata.db which already
summarizes most metadata from all tracks. See for instance
<a href="https://github.com/tb2332/MSongsDB/blob/master/Tasks_Demos/SQLite/list_all_artists_from_db.py">
list_all_artists_from_db.py</a>.
<p>

<a name='preview'><h3>Preview audio</h3></a>
The dataset does not come with audio, but there are many services out
there that provide audio samples for free (at least a few thousand per day).
We use such a service, <a href='http:www.7digital.com'>7digital</a>, 
and the different 7digital ID (artist, release, track) are already in 
the dataset.<br>
The following 
<a href='https://github.com/tb2332/MSongsDB/blob/master/Tasks_Demos/Preview7digital/get_preview_url.py'>
python code</a> takes a HDF5 song file and
looks for a preview. It outputs the URL. It first look for the track ID
if we have it. If we don't, but we have the ID for the release or the
artist, it pull all song associated with these and check for the closest
match. If not, it uses 7digital API to search by 'artist name' + 
'track name'.<p>
<small>
<div style="border:1px solid #808080;padding:1.2em; background-color:#f0f0f0; margin-left: 25px; margin-right: 40px;">
<div style="margin-left: 25px;">
<pre>
python get_preview_url.py HDF5file
python get_preview_url.py -7digitalkey 98sdjwdd HDF5file
</pre>
</div>
</div>
</small>
<p>

</div>
